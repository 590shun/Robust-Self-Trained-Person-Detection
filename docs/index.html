

<!DOCTYPE html>
<html>
<head>
	<title>Robust Self-Trained Person Detection for Vulnerable Road Users </title>
    <link rel="stylesheet" type="text/css" href="./pvg.css">
    <link rel="shortcut icon" type="image/png" href="./img/cc_logo_1_crop.png">
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
</head>

<body>
<script type="text/javascript" src="./header.js"></script>

<style>
a.myclass {
    color:#DE382D;
    text-decoration: underline
}
</style>

<style>
a.link {
    text-decoration: underline
}
</style>


<h1 align="center" style="font-size: 30pt;"><b>Robust Self-Trained Person Detection for Vulnerable Road Users </b></h1><br/>

<center>
    <font color="#c7254e">Conference on Computer Vision and Pattern Recognition (CVPR) 2021<br><b></b><br></font><br><br>
    <a href="http://twitter.com/k0gu_chan" class="">Shunsuke Kogure</a><sup>1,3</sup> &emsp; Kai Watabe</a><sup>2,3</sup> &emsp;  <a href="https://twitter.com/FragileGoodwill" clss="">Ryosuke Yamada<sup>2,3</sup> &emsp; Yoshimitsu Aoki<sup>4</sup><br>&emsp; <a href="https://mmai.tech/" class="">Nakamasa Inoue</a><sup>4</sup> &emsp; <a href="http://www.is.fr.dendai.ac.jp/"  class="">Akio Nakamura</a><sup>2</sup> &emsp; <a href="https://staff.aist.go.jp/yu.satou/"  class="">Yutaka Satoh</a><sup>1,3</sup><br>
    1: AIST &emsp; 2: TDU &emsp; 3: Univ. of Tsukuba &emsp; 4: TITech<br><br>
    <a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Kataoka_Pre-training_without_Natural_Images_ACCV_2020_paper.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Paper</a>
    <a href="https://github.com/hirokatsukataoka16/FractalDB" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Code</a>
    <a href="#dataset" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Dataset</a>
    <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_oral.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Oral</a>
    <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_poster.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Poster</a>
    <a href="http://hirokatsukataoka.net/pdf/accv20_kataoka_fractaldb_supplementary.pdf" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Supp. Mat.</a>
    <a href="https://hirokatsukataoka16.github.io/Vision-Transformers-without-Natural-Images/" class="btn btn-secondary btn-lg active" role="button" aria-pressed="true">Related Work</a>
    <br><br>
    <!--<img src="./img/teaser.png" style="width: 100%;"/>-->
</center>

<br>
<h2>Abstract</h2>
<p>
Pedestrian detection is an expected function in automaticdriving and other applications. However, there should be no disparities in miss rates between sensitive attributes, such as age and gender. In this paper, we examine this issue by efficiently expanding and self-training a large-scale person dataset. Specifically, we apply the Weakly-Supervised Person Dataset (WSPD), a pre-trained pedestrian detection network, to the database Places365 to efficiently collect pedestrian data. We also investigate the miss rate disparities between sensitive attributes in the conventional pre-trained model by manually re-annotating bounding boxes for “adult”, “child” and “elderly” attributes in the INRIA Person Dataset. We then collect 3,461,024 images and 9,739,996 bounding boxes from the ‘Self-TrainedPedestrian Dataset (STPD)’. Our pre-trained detector successfully improves the miss rate for adult by up to 9.2%, for children by up to 8.5% and for the elderly by up to 6.5% over the baseline model. However, it should be rememberedthat there is a difference in the effects of improvement in the expansion of the dataset by self-learning depending on the attributes.
</p>

<br>
<h2>Framework</h2>
Proposed pre-training without natural images based on fractals, which is a natural formula existing in the real world (Formula-driven Supervised Learning). We automatically generate a large-scale labeled image dataset based on an iterated function system (IFS). (Bottom-left image)
The pre-training framework with Fractal geometry for feature representation learning. We can enhance natural image recognition by pre-training without natural images. (Bottom-right image) Accuracy transition among ImageNet-1k, FractalDB-1k and training from scratch.
<br><br><br>
<center>
        <img src="./img/framework.png" style="width: 100%;"/>
</center>

<br><br><br>
<h2>Experimental Results</h2>

We compared Scratch from random parameters, Places-30/365, ImageNet-100/1k (ILSVRC’12), and FractalDB-1k/10k in the following table. Since our implementation is not completely the same as a representative learning configuration, we implemented the framework fairly with the same parameters and compared the proposed method (FractalDB-1k/10k) with a baseline (Scratch, DeepCluster-10k, Places-30/365, and ImageNet-100/1k). The  proposed  FractalDB  pre-trained  model  recorded  several  good  performance rates. We respectively describe them by comparing our Formula-driven Supervised Learning with Scratch, Self-supervised and Supervised Learning.
<br>
<center>
        <img src="./img/results.png" style="width: 85%;"/>
</center>


<br><br><br>
<h2>Visual Results</h2>

The figures show the activation of the 1st convolutional layer
on ResNet-50 at each pre-training model.
<br>
<center>
        <img src="./img/conv1.png" style="width: 85%;"/>
</center>

<br>
<h2>Citation</h2>
@inproceedings{KataokaACCV2020,<br>
&emsp;author     = {Kataoka, Hirokatsu and Okayasu, Kazushige and Matsumoto, Asato and Yamagata, Eisuke and Yamada, Ryosuke and Inoue, Nakamasa and Nakamura, Akio and Satoh, Yutaka},<br>
&emsp;title      = {Pre-training without Natural Images},<br>
&emsp;journal    = {Asian Conference on Computer Vision (ACCV)},<br>
&emsp;year       = {2020}<br>
}
<br><br>

<a name="dataset"><h2>Dataset Download</h2></a>
<ul>
    <li>
        FractalDB-1k (1k categories x 1k instances; Total 1M images).
        <a href="">[Dataset (13GB)]</a>
    </li>
    <li>
        FractalDB-60 (60 well-known categories x 1k instances; Total 60k images).
        <a href="https://drive.google.com/file/d/1F0aEogTScpABjJhNZJaCFT-J8mkdP86o/view?usp=sharing">[Dataset (1.2GB)]</a>
    </li>
</ul>
<br><br>

<h2>Acknowledgement</h2></a>
<ul>
    <li> This work was supported by JSPS KAKENHI Grant Number JP19H01134.</li>
    <li> Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided by National Institute of Advanced Industrial Science and Technology (AIST) was used.</li>
</ul>

<br><br><br>
<script type="text/javascript" src="./footer.js"></script>
</body></html>
